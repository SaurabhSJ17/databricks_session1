{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62baae16-372d-42b3-86bc-66e35f434394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SIMPLY LOAD DATA FROM SOURCE TO DESTINATION \n",
    "## FAILS WHEN NEW COLUMN DETECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e61257-8906-4bd8-a45e-46b2e380e5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.format('cloudFiles')\\\n",
    "    .option(\"cloudFiles.format\", \"csv\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"dbfs:/mnt/autofolder/data/schema\")\\\n",
    "    .load(\"dbfs:/mnt/autofolder/data\")\\\n",
    "    .writeStream\\\n",
    "    .option('mergeSchema', 'true')\\\n",
    "    .option('checkpointLocation', \"dbfs:/mnt/autofolder/output_data/checkpoint\")\\\n",
    "    .option('path', \"dbfs:/mnt/autofolder/output_data/\")\\\n",
    "    .toTable(\"new_autoloader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e6d937-c330-43ae-ad3d-d8699aa483f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## allow new column but save  this columns  in table as rescued data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79939219-7d88-46bf-92fb-cd2256e8355e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.format('cloudFiles')\\\n",
    "    .option(\"cloudFiles.format\", \"csv\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"dbfs:/mnt/autofolder/data/schema\")\\\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\",\"rescue\")\\\n",
    "    .option('rescuedDataColumn',\"_rescued_data\")\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load(\"dbfs:/mnt/autofolder/data\")\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e33f069d-3857-4dfb-8c99-47cc98528679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.writeStream\\\n",
    "    .option('mergeSchema', 'true')\\\n",
    "    .option('checkpointLocation', \"dbfs:/mnt/autofolder/output_data/checkpoint\")\\\n",
    "    .option('path', \"dbfs:/mnt/autofolder/output_data/\")\\\n",
    "    .toTable(\"new_autoloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51e2945-f5c7-46b6-99ed-568c2c5cbaab",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754390620723}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "474ce0ec-e7de-4185-af64-6b0c7308a687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Backfill Old Rescued Data into New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311f4510-eee7-46be-989b-20b149a7cb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "\n",
    "# Step 1: Read the Delta table with rescued data\n",
    "df_existing = spark.read.format(\"delta\").option(\"header\", \"true\").load(\"dbfs:/mnt/autofolder/output_data/\")\n",
    "\n",
    "# Step 2: Define schema for rescued JSON\n",
    "rescued_schema = StructType() \\\n",
    "    .add(\"phone_no\", StringType()) \\\n",
    "\n",
    "# Step 3: Parse the JSON inside _rescued_data\n",
    "df_parsed = df_existing.withColumn(\n",
    "    \"rescued_parsed\",\n",
    "    from_json(col(\"_rescued_data\"), rescued_schema)\n",
    ")\n",
    "\n",
    "# # Step 4: Extract fields into real columns\n",
    "df_extracted = df_parsed \\\n",
    "    .withColumn(\"phone_no\", col(\"rescued_parsed.phone_no\")) \\\n",
    "    .drop(\"rescued_parsed\")\n",
    "\n",
    "# Initialize the directory as a Delta table if it is not already\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS updated_rescued_data USING DELTA LOCATION 'dbfs:/mnt/autofolder/output_data/updated_rescued_data'\")\n",
    "\n",
    "# df_extracted.show()\n",
    "# # Step 5: Overwrite Delta table with merged schema\n",
    "df_extracted.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"dbfs:/mnt/autofolder/output_data/updated_rescued_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b91fae2b-2f77-44cd-908d-dc990f26ac7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Incremental Backfill of Rescued Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f53cd282-4292-4302-aa25-42393e055daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, lit\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "\n",
    "# Read existing Delta table data\n",
    "df_existing = spark.read.format(\"delta\").load(\"dbfs:/mnt/autofolder/output_data/\")\n",
    "\n",
    "# Filter rows where _rescued_data is NOT null (only these need backfill)\n",
    "df_to_fix = df_existing.filter(col(\"_rescued_data\").isNotNull())\n",
    "\n",
    "# Define schema for rescued JSON fields\n",
    "rescued_schema = StructType() \\\n",
    "    .add(\"city\", StringType()) \\\n",
    "    .add(\"country\", StringType())\n",
    "\n",
    "# Parse the JSON in _rescued_data column\n",
    "df_parsed = df_to_fix.withColumn(\n",
    "    \"rescued_parsed\",\n",
    "    from_json(col(\"_rescued_data\"), rescued_schema)\n",
    ")\n",
    "\n",
    "# Extract fields as proper columns\n",
    "df_extracted = df_parsed \\\n",
    "    .withColumn(\"city\", col(\"rescued_parsed.city\")) \\\n",
    "    .withColumn(\"country\", col(\"rescued_parsed.country\")) \\\n",
    "    .drop(\"rescued_parsed\")\n",
    "\n",
    "# Get rows where _rescued_data is null to keep them unchanged\n",
    "df_unchanged = df_existing.filter(col(\"_rescued_data\").isNull())\n",
    "\n",
    "# Union fixed rows with unchanged rows\n",
    "df_final = df_extracted.unionByName(df_unchanged)\n",
    "\n",
    "# Overwrite Delta table with merged schema (only fixed rows updated)\n",
    "df_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"dbfs:/mnt/autofolder/output_data/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dade18b8-b1c3-48df-945c-f6e8733e3f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SCHEMA EVOLUTION DIRECTLY HANDLING INCREMENTAL LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53dc8946-85d5-4614-91e8-80a00eac5586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"dbfs:/mnt/autofolder/data/schema\")   # must stay consistent\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")                # enables new column addition\n",
    "        .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")                   # capture anything unmatched\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"dbfs:/mnt/autofolder/data/\")\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebff7876-c16f-4be6-b6f7-aebc185fbaf8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754395875250}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db7a572e-e85f-4be0-9cd8-730f61ac71e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.writeStream.format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/mnt/autofolder/output_data/_check_point\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start(\"dbfs:/mnt/autofolder/output_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28763e7-896b-47d9-b4c5-05b899924940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.format(\"delta\").load(\"dbfs:/mnt/autofolder/output_data/\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_auto_loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
